{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKXnCpx7o9Iw"
      },
      "outputs": [],
      "source": [
        "# Got CUDA out of memory errors due to fragmentation, used this line to fix the issue\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess Data"
      ],
      "metadata": {
        "id": "vaXT4SwJpNbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bullet_list1 = \"\"\"this paper:\n",
        "  - uses computational, declarative approach to prosodic morphology\n",
        "  - uses inviolable constraints to create small candidate sets\n",
        "  - filtered by incremental optimization\n",
        "  - illustrated with modern hebrew verbs\n",
        "  - novel off-line technique to reduce run-time optimization\n",
        "  - allows for more expressivity in constraints\"\"\"\n",
        "\n",
        "par1 = \"The paper presents a novel computational and declarative approach to prosodic morphology. It utilizes inviolable constraints to generate small candidate sets that are then filtered by an incremental optimization process. The approach is illustrated using modern Hebrew verbs as an example. The paper also introduces a new off-line technique that reduces the run-time optimization, resulting in faster processing. The proposed method allows for more expressivity in constraints, leading to improved accuracy and efficiency in the analysis of prosodic morphology.\"\n",
        "\n",
        "bullet_list2 = \"\"\"  - paper reviews expectation-maximization algorithm\n",
        "  - section 2: relative-frequency & maximum-likelihood estimation\n",
        "  - section 3: expectation-maximization & a simpler variant, generalized expectation-maximization\n",
        "  - section 4: example of two loaded dice\"\"\"\n",
        "\n",
        "par2 = \"The paper reviews the expectation-maximization algorithm and is organized into several sections. In section 2, the author discusses relative-frequency and maximum-likelihood estimation. Section 3 covers the expectation-maximization algorithm and a simpler variant known as generalized expectation-maximization. Finally, in section 4, the author provides an example of two loaded dice. Overall, the paper provides a comprehensive overview of the expectation-maximization algorithm and its applications in statistics and machine learning.\"\n",
        "\n",
        "bullet_list3 = \"\"\" the takeaways:\n",
        "  - huge language models provide superior few-shot performance on downstream tasks\n",
        "  - encoder-decoder models & multi-task prompt tuning has:\n",
        "    - better downstream task performance\n",
        "    - zero-shot generalizability\n",
        "  - RLHF (Reinforcement Learning with Human Feedback) finetuning:\n",
        "    - improves generative output quality\n",
        "    - keeps model size small\"\"\"\n",
        "\n",
        "par3 = \"Huge language models have been shown to provide superior few-shot performance on downstream tasks. This is especially true for encoder-decoder models and multi-task prompt tuning, which not only improve downstream task performance but also have zero-shot generalizability. In addition, Reinforcement Learning with Human Feedback (RLHF) finetuning has been found to improve generative output quality while keeping the model size small. Overall, these takeaways demonstrate the immense potential of using large language models for a wide range of natural language processing tasks, with various techniques and approaches available for further optimization and improvement.\"\n",
        "\n",
        "bullet_list4 = \"\"\"Evaluation Metrics should be:\n",
        "  - low cost: should be affordable\n",
        "  - tunable: need to optimize system towards evaluation metric\n",
        "  - meaningful: should give intuitive interpretation of translation quality\n",
        "  - consistent: repetitive use of metric should give same results\n",
        "  - correct: metric must rank better systems higher\"\"\"\n",
        "\n",
        "par4 = \"Evaluation metrics are an important aspect of any translation system. In order to effectively evaluate the quality of translation, the metrics used must meet certain criteria. Firstly, the cost of these metrics must be low and affordable. Secondly, they should be tunable, allowing for the optimization of the system towards the evaluation metric. Furthermore, the metrics must be meaningful, providing an intuitive interpretation of translation quality. Consistency is also crucial, as repetitive use of the metric should yield the same results. Finally, the metric must be correct and rank better systems higher, ensuring that accurate and reliable evaluations are obtained. By meeting these criteria, evaluation metrics can provide valuable insights into the quality of translation systems, allowing for their continuous improvement and development.\"\n",
        "\n",
        "bullet_list5 = \"\"\"The benefits of learning a new language:\n",
        "  - Connect with more people personally and professionally\n",
        "  - Enhance job opportunities and potential for career advancement\n",
        "  - Learn about new cultures and broaden your perspective\n",
        "  - Improve cognitive abilities, memory, problem-solving, and multitasking skills\n",
        "  - Enhance travel experiences and connect with locals more meaningfully\n",
        "  - Foster personal growth and build self-confidence\n",
        "  - Keep the brain active and potentially reduce the risk of cognitive decline and dementia\n",
        "  - Access new sources of information that may not be available in your native language\"\"\"\n",
        "\n",
        "par5 = \"Learning a new language has many benefits. Firstly, it can help you communicate with a wider range of people, both professionally and personally. Multilingual individuals are highly valued in the job market and can open up opportunities for career advancement and global work. Additionally, learning a new language can expose you to new cultures and ways of thinking, enhancing your appreciation and understanding of the world. It can also improve cognitive abilities, such as memory, problem-solving, and multitasking skills. When travelling, knowing a new language can make your experiences more enjoyable and enriching, allowing you to connect with locals and experience their culture in a more meaningful way. Furthermore, learning a new language can be a challenging and rewarding experience that can help build self-confidence and foster personal growth. Studies even show that learning a new language can help keep the brain active and potentially reduce the risk of cognitive decline and dementia later in life. Lastly, it can open up new sources of information, such as books, articles, and websites, that may not be available in your native language. All in all, the benefits of learning a new language are numerous and can positively impact various areas of your life.\"\n",
        "\n",
        "bullet_list6 = \"\"\"'Top Gun' is a great movie because\n",
        "  - Iconic soundtrack: featuring a memorable soundtrack filled with classic 80s songs\n",
        "  - Intense aerial action scenes: thrilling and visually stunning aerial combat scenes\n",
        "  - Memorable performances: delivering memorable performances, particularly by Tom Cruise and Val Kilmer\n",
        "  - Timeless story: themes of friendship, love, and the pursuit of excellence that resonate with audiences of all ages\n",
        "  - Cultural impact: there are references and parodies in films, TV shows, and even video games\n",
        "  - Nostalgia factor: reminding viewers of the 80s, with its fashion, technology, and overall aesthetic\n",
        "  - Popularity: a large fan base and lasting appeal since its release in 1986\"\"\"\n",
        "\n",
        "par6 = \"'Top Gun' is considered a great movie for several reasons. Firstly, it features an iconic soundtrack that includes classic 80s songs. Additionally, the film boasts intense and visually stunning aerial combat scenes that are sure to thrill viewers. Memorable performances, especially by Tom Cruise and Val Kilmer, are also a hallmark of the movie. Furthermore, 'Top Gun' tells a timeless story with themes of friendship, love, and the pursuit of excellence that resonates with audiences of all ages. The film has had a significant cultural impact, inspiring references and parodies in films, TV shows, and even video games. The movie also provides viewers with a sense of nostalgia, showcasing the fashion, technology, and overall aesthetic of the 80s. Finally, 'Top Gun' has a large fan base and continues to enjoy lasting appeal since its release in 1986.\"\n",
        "\n",
        "bullet_list7 = \"\"\"CAR-T cells are:\n",
        "  - genetically modified immune cells for cancer immunotherapy\n",
        "  - CAR-T stands for \"chimeric antigen receptor T cell\"\n",
        "  - First: T cells are taken from patient's blood and modified to express CAR on surface\n",
        "  - Next: CAR recognizes and binds to cancer cell protein\n",
        "  - Then: Infused CAR-T cells travel to cancer site and attack\n",
        "  - CAR-T therapy = promising for certain blood cancers\n",
        "  - Complex therapy, needs careful management to minimize side effects.\"\"\"\n",
        "\n",
        "par7 = \"CAR-T cells are a type of genetically modified immune cells used in cancer immunotherapy. The name 'CAR-T' stands for 'chimeric antigen receptor T cell.' To create CAR-T cells, T cells are taken from a patient's blood and modified in a laboratory to express CAR on their surface. This receptor recognizes and binds to a specific protein found on cancer cells. Once the CAR-T cells are infused back into the patient's bloodstream, they can travel to the site of the cancer and attack the cancer cells. CAR-T cell therapy has shown promise in treating certain types of blood cancers. However, this therapy is complex and requires careful management to minimize potential side effects.\"\n",
        "\n",
        "bullet_list8 = \"\"\"Viterbi decoding:\n",
        "  - An algorithm used to decode hidden states in an HMM\n",
        "  - Observed data are generated by hidden states\n",
        "  - Dynamic programming to compute likelihood of each possible sequence\n",
        "  - Computing the most likely path to each state at each time step\n",
        "  - Selecting the most likely path and continuing the process\n",
        "  - Backtracking through the most likely path to determine the sequence of hidden states\n",
        "  - Wide usage in speech recognition, natural language processing, and bioinformatics.\"\"\"\n",
        "\n",
        "par8 = \"Viterbi decoding is an algorithm that uses dynamic programming to decode the most likely sequence of hidden states in a hidden Markov model. The algorithm computes the most likely path to each possible state at each time step and backtracks through the most likely path to determine the sequence of hidden states that generated the observed data. Widely used in speech recognition, natural language processing, and bioinformatics, Viterbi decoding is a powerful tool for decoding hidden state sequences.\"\n",
        "\n",
        "bullet_list9 = \"\"\"In the poem 'The Raven' by Edgar Allen Poe:\n",
        "  - Raven symbolizes death and loss\n",
        "  - 'Nevermore' repeated by the raven symbolizes finality and hopelessness\n",
        "  - Lenore represents the narrator's lost love\n",
        "  - The bust of Pallas Athena symbolizes wisdom and knowledge\n",
        "  - Darkness and night symbolize sorrow and melancholy\n",
        "  - Chamber door symbolizes the boundary between life and death\n",
        "  - Knocking at the door symbolizes narrator's fear of death and the unknown\n",
        "  - The raven's perch on bust of Pallas symbolizes triumph of death over wisdom and knowledge\"\"\"\n",
        "\n",
        "par9 = \"In Edgar Allan Poe's poem, 'The Raven,' the raven symbolizes death and loss, while the repeated word 'Nevermore' represents finality and hopelessness. The narrator's lost love is personified by Lenore, and the bust of Pallas Athena symbolizes wisdom and knowledge. Darkness and night are used to convey sorrow and melancholy, while the chamber door represents the boundary between life and death. The knocking at the door symbolizes the narrator's fear of death and the unknown. Ultimately, the raven's perch on the bust of Pallas represents the triumph of death over wisdom and knowledge.\"\n",
        "\n",
        "bullet_list10 = \"\"\"this paper:\n",
        "  - proposes a new architecture called Transformer that relies solely on attention mechanisms, eliminating the need for recurrence and convolutions.\n",
        "  - Transformer is superior in quality, more parallelizable, and requires less time to train\n",
        "  - achieves state-of-the-art results on two machine translation tasks\n",
        "  - establishes a new single-model state-of-the-art BLEU score after training for 3.5 days on eight GPUs.\n",
        "  - generalizes well to other tasks, shown by its successful application to English constituency parsing\"\"\"\n",
        "\n",
        "par10 = \"This paper proposes a new architecture called Transformer that is based entirely on attention mechanisms, eliminating the need for recurrence and convolutions. The Transformer model is shown to be superior in quality, more parallelizable, and faster to train than existing models. The model achieves state-of-the-art results on two machine translation tasks, including establishing a new single-model state-of-the-art BLEU score after training for 3.5 days on eight GPUs. Additionally, the Transformer model generalizes well to other tasks, as demonstrated by its successful application to English constituency parsing.\"\n",
        "\n",
        "bullet_list11 = \"\"\"Why you should study AI:\n",
        "  - AI skills in high demand across healthcare, finance, transportation, and e-commerce\n",
        "  - Diverse AI career opportunities from AI Engineer to Research Scientist\n",
        "  - Studying AI fosters critical thinking, problem-solving, and data analysis skills\n",
        "  - AI = rapidly evolving field, offering chances to engage in cutting-edge research and develop new AI technologies\n",
        "  - it has potential to impact areas such as healthcare, education, and climate change for the better\"\"\"\n",
        "\n",
        "par11 = \"'There are several reasons why you should consider studying AI. Firstly, AI skills are highly sought after across a range of industries including healthcare, finance, transportation, and e-commerce. This translates into a wide variety of career opportunities, ranging from AI Engineer to Research Scientist. Moreover, studying AI fosters critical thinking, problem-solving, and data analysis skills that are highly valued in today's job market. As an added benefit, AI is a rapidly evolving field that offers opportunities to engage in cutting-edge research and develop new AI technologies. Finally, the potential impact of AI on areas such as healthcare, education, and climate change is enormous, and studying AI can help prepare you to make a positive contribution in these areas.\"\n",
        "\n",
        "bullet_list12 = \"\"\"Kefir:\n",
        "  - a fermented dairy product consumed for centuries around the world\n",
        "  - made from milk and kefir grains, a symbiotic mixture of bacteria and yeasts\n",
        "  - Kefir grains convert lactose, the main sugar in milk, into lactic acid during the fermentation process, which gives kefir its tangy taste\n",
        "  - rich in probiotics, which can improve gut health and aid in digestion\n",
        "  - versatile\n",
        "  - can be consumed on its own or used as a base for various culinary creations, like smoothies and salad dressings.\"\"\"\n",
        "\n",
        "par12 = \"Kefir is a fermented dairy product that has been consumed for centuries in various parts of the world. It is typically made from milk, which is inoculated with kefir grains, a symbiotic mixture of bacteria and yeasts. During the fermentation process, the kefir grains convert lactose, the main sugar in milk, into lactic acid, which gives kefir its characteristic tangy taste. Kefir is known for its probiotic properties, as it contains a diverse array of beneficial microorganisms that can aid in digestion and improve overall gut health. In addition to its nutritional benefits, kefir is also prized for its versatility and can be enjoyed on its own or used as a base for smoothies, salad dressings, and other culinary creations.\"\n",
        "\n",
        "bullet_list13 = \"\"\"Benefits of drinking tea:\n",
        "  - Tea is rich in antioxidants that protect the body from damage by free radicals\n",
        "  - Reduced risk of chronic diseases: Regular tea consumption is linked to a lower risk of heart disease, type 2 diabetes, and some cancers\n",
        "  - Tea contains caffeine and L-theanine, which can boost brain function and enhance focus\n",
        "  - Tea is a hydrating beverage that can help meet daily fluid needs\n",
        "  - Drinking tea may help with weight management by increasing metabolism and promoting fat burning\"\"\"\n",
        "\n",
        "par13 = \"Drinking tea comes with a host of benefits for our bodies. One of the most notable benefits is its abundance of antioxidants, which help protect our bodies from damage caused by free radicals. Additionally, regular tea consumption has been linked to a reduced risk of chronic diseases such as heart disease, type 2 diabetes, and certain cancers. Moreover, tea contains caffeine and L-theanine, which have been shown to improve brain function and enhance focus. Another benefit of drinking tea is that it can help with hydration, making it a great way to meet our daily fluid needs. Finally, drinking tea may prove beneficial for weight management by increasing metabolism and promoting fat burning.\"\n",
        "\n",
        "bullet_list14 = \"\"\"Omega-3 fatty acids:\n",
        "  - Omega-3s have anti-inflammatory properties that help to reduce inflammation in the body\n",
        "  - can help to reduce triglycerides, lower blood pressure, and reduce the risk of heart disease and stroke\n",
        "  - Omega-3s are essential for brain development and function\n",
        "  - may help to improve cognitive function and memory\n",
        "  - reduce symptoms of depression and anxiety by regulating neurotransmitters and reducing inflammation in the brain\n",
        "  - Omega-3s are important for eye health and can help prevent age-related macular degeneration\"\"\"\n",
        "\n",
        "par14 = \"Omega-3 fatty acids provide a range of benefits to the body. Their anti-inflammatory properties are effective in reducing inflammation throughout the body. Additionally, they can reduce triglycerides, lower blood pressure, and decrease the risk of heart disease and stroke. Omega-3s are crucial for proper brain development and function, and they may even improve cognitive function and memory. Moreover, these fatty acids can reduce symptoms of depression and anxiety by regulating neurotransmitters and reducing inflammation in the brain. Lastly, Omega-3s play an important role in maintaining eye health and preventing age-related macular degeneration.\"\n",
        "\n",
        "bullet_list15 = \"\"\"Text style transfer:\n",
        "  - a crucial part of natural language generation\n",
        "  - goal is to control specific attributes in generated text, like humor, politeness, and emotion\n",
        "  - this task has been around for a long time in the field of natural language processing\n",
        "  - recently: deep neural models brought significant attention to this task\n",
        "  - the paper presents a systematic survey of over 100 articles on neural text style transfer\n",
        "  - it covers task formulation, existing datasets, evaluation, and methodologies in the presence of parallel and non-parallel data\n",
        "  - also discusses future developments in the field\"\"\"\n",
        "\n",
        "par15 = \"Text style transfer is a crucial aspect of natural language generation that has been a part of the field of natural language processing for a long time. The goal of this task is to control specific attributes in generated text, such as humor, politeness, and emotion. However, with the recent advancements in deep neural models, there has been a significant increase in attention towards this task. In this paper, the authors present a systematic survey of over 100 articles on neural text style transfer, covering task formulation, existing datasets, evaluation, and methodologies in the presence of both parallel and non-parallel data. Furthermore, the authors discuss potential future developments in this field.\""
      ],
      "metadata": {
        "id": "bw5LwxsYpGDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sort data by inputs, labels\n",
        "inputs = [bullet_list1,bullet_list2,bullet_list3,bullet_list4,bullet_list5,bullet_list6,bullet_list7,bullet_list8,bullet_list9,bullet_list10,bullet_list11,bullet_list12,bullet_list13,bullet_list14,bullet_list15]\n",
        "labels = [par1,par2,par3,par4,par5,par6,par7,par8,par9,par10,par11,par12,par13,par14,par15]"
      ],
      "metadata": {
        "id": "m5jqCnXqtNVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "# n=10 for train, n=5 for eval\n",
        "train_dataset = Dataset.from_pandas(pd.DataFrame({'inputs': inputs[0:10], 'labels': labels[0:10]}))\n",
        "test_dataset = Dataset.from_pandas(pd.DataFrame({'inputs': inputs[10:], 'labels': labels[10:]}))"
      ],
      "metadata": {
        "id": "Z3sorI4wpLKv",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "0f0dfd96-375d-4f2e-af2b-7c6d1ff9c02e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-627efb942b81>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'text_inputs'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'text_labels'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'datasets'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = DatasetDict({'train':train_dataset,'test':test_dataset})"
      ],
      "metadata": {
        "id": "8zshMisvpMHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in model and relevant hyperparameters"
      ],
      "metadata": {
        "id": "-_ZZw3ZppUxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "UOlgCluepL-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using T5 small model\n",
        "# NOTE: the code in this cell also manually adds <BPT> to the tokenizer and model. Remove if you want to train/predict using hyphens instead of the <BPT> tag\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "special_tokens_dict = {'additional_special_tokens': ['<BPT>']}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "oqBTmdZUpL8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing \n",
        "\n",
        "We took guidance from the HF T5 Tutorials - https://huggingface.co/docs/transformers/model_doc/t5"
      ],
      "metadata": {
        "id": "XkwdbS4speIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prefix we chose follows the format and style shown in the original T5 paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "# (see their Appendix D for examples)\n",
        "prefix = \"rewrite as a paragraph: \""
      ],
      "metadata": {
        "id": "cVi2XmHWpL5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define max input,target lengths\n",
        "max_length = 400"
      ],
      "metadata": {
        "id": "ps2WMhCjpL3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "# For comparing lists with hyphens vs. lists with bullet point tag (<BPT>)\n",
        "# this fn converts all series of white space followed by a hyphen into a <BPT> tag\n",
        "def preprocess_list(text):\n",
        "  text = re.sub('\\s+- ',' <BPT> ',text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "UyRR-1C-s74x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_inputs(data):\n",
        "  return tokenizer([prefix + preprocess_list(input) for input in data['inputs']], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
        "\n",
        "def preprocess_labels(data):\n",
        "  encoding = tokenizer(data['labels'], padding='max_length', max_length=max_length, truncation=True, return_tensors='pt')\n",
        "  return {'labels': encoding['input_ids']}"
      ],
      "metadata": {
        "id": "ZaWMcX-ypL0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess_inputs,batched=True)\n",
        "dataset = dataset.map(preprocess_labels,batched=True)"
      ],
      "metadata": {
        "id": "C7-enqCQpLyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into trn, dev, and test datasets. Note that we're not using all available data for the sake of time\n",
        "train_dataset = dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "dev_dataset = dataset['dev'].shuffle(seed=42).select(range(625))\n",
        "test_dataset = dataset['test'].shuffle(seed=42).select(range(625))"
      ],
      "metadata": {
        "id": "mm6ygq2NpLvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetune model"
      ],
      "metadata": {
        "id": "Vo1T9scip5jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ],
      "metadata": {
        "id": "jrXClJtppLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "89bd1kCdpLjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "import textstat\n",
        "\n",
        "# compute rouge, bleu, and readability. Also computes readability of dataset for comparison purposes\n",
        "def my_compute_metrics(eval_pred):\n",
        "  labels = eval_pred.label_ids\n",
        "  pred_ids = eval_pred.predictions\n",
        "  if isinstance(pred_ids, tuple):\n",
        "    pred_ids = pred_ids[0]\n",
        "\n",
        "  pred_ids = np.argmax(pred_ids, axis=-1)\n",
        "\n",
        "  # Decode predictions to compute metrics\n",
        "  preds_text = tokenizer.batch_decode(pred_ids, skip_special_tokens=True)\n",
        "\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  labels_text = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  preds_text, labels_text = postprocess_text(preds_text, labels_text)\n",
        "  \n",
        "  # init metrics\n",
        "  metric1 = evaluate.load('rouge')\n",
        "  metric2 = evaluate.load('bleu')\n",
        "\n",
        "  # compute metrics\n",
        "  metrics = metric1.compute(predictions=preds_text, references=labels_text)\n",
        "  metrics['bleu'] = metric2.compute(predictions=preds_text, references=labels_text)['bleu']\n",
        "  metrics['readability'] = sum([textstat.flesch_kincaid_grade(x) for x in preds_text]) / len(preds_text) #FKGL score\n",
        "  metrics['label_readability'] = sum([textstat.flesch_kincaid_grade(x[0]) for x in labels_text]) / len(labels_text)\n",
        "\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "mIJiexzSp-_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "train_args = TrainingArguments(\n",
        "  output_dir=\"finetune_bpt\",\n",
        "  run_name='run_6_train_bpt',\n",
        "  remove_unused_columns=True,\n",
        "  num_train_epochs=20,\n",
        "  learning_rate=0.0005, \n",
        "  logging_steps=5,\n",
        "  do_train=True,\n",
        "  do_predict=False,\n",
        "  do_eval=False,\n",
        "  fp16=True,\n",
        "  per_device_train_batch_size=1\n",
        ")"
      ],
      "metadata": {
        "id": "7cJkZ94IpLml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init test trainer\n",
        "trainer = Trainer(\n",
        "            model=model, \n",
        "            args=train_args, \n",
        "            train_dataset=dataset['train'],\n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=my_compute_metrics)"
      ],
      "metadata": {
        "id": "h-9mq_7Bp-8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Inqoo27Pp-5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save finetuned model and tokenizer\n",
        "trainer.save_model('best_model_bpt')\n",
        "tokenizer.save_pretrained('best_model_bpt')\n",
        "\n",
        "model = trainer.model"
      ],
      "metadata": {
        "id": "iq2WklanqUoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model"
      ],
      "metadata": {
        "id": "Ab36LG5EqSbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval finetuned model in inference mode\n",
        "test_args = TrainingArguments(\n",
        "    output_dir=\"finetune_para_eval\",\n",
        "    do_train=False,\n",
        "    do_predict=True,\n",
        "    per_device_eval_batch_size=4,   \n",
        ")"
      ],
      "metadata": {
        "id": "9ttBkf28pLdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init test trainer\n",
        "test_trainer = Trainer(\n",
        "            model=model, \n",
        "            args=test_args, \n",
        "            tokenizer=tokenizer,\n",
        "            data_collator=data_collator,\n",
        "            compute_metrics=my_compute_metrics)"
      ],
      "metadata": {
        "id": "mtrEIQ0dqM9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = test_trainer.evaluate(dataset['test'])\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "mhZQsuQuqNSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}