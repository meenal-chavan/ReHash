{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKXnCpx7o9Iw"
      },
      "outputs": [],
      "source": [
        "# Got CUDA out of memory errors due to fragmentation, used this line to fix the issue\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in data\n",
        "\n",
        "Data can be found here: https://cs.pomona.edu/~dkauchak/simplification/"
      ],
      "metadata": {
        "id": "vaXT4SwJpNbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tarfile\n",
        "f = tarfile.open('sentence-aligned.v2.tar.gz')\n",
        "f.extractall('./data')\n",
        "f.close()"
      ],
      "metadata": {
        "id": "bw5LwxsYpGDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_dataset(path):\n",
        "  data = []\n",
        "  f = open(path, 'r')\n",
        "  lines = f.readlines()\n",
        "  for line in lines:\n",
        "    data.append(line)\n",
        "  return data"
      ],
      "metadata": {
        "id": "Z3sorI4wpLKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using ver. 2.0 of the dataset, where each datapoint is of the format:\n",
        "#   TOPIC \\t NUMBER \\t SENTENCE\n",
        "x = get_dataset('./data/sentence-aligned.v2/normal.aligned')\n",
        "y = get_dataset('./data/sentence-aligned.v2/simple.aligned')"
      ],
      "metadata": {
        "id": "8zshMisvpMHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract sentences\n",
        "x = [s.split('\\t')[2] for s in x]\n",
        "y = [s.split('\\t')[2] for s in y]"
      ],
      "metadata": {
        "id": "JMvhLTKipMEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "import pandas as pd\n",
        "import regex as re\n",
        "\n",
        "data = {'text_inputs':x, 'text_labels':y}\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(data=data))"
      ],
      "metadata": {
        "id": "Wfrl0352pMBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out samples where input and target are the same\n",
        "dataset = dataset.filter(lambda x: x['text_inputs'] != x['text_labels'])"
      ],
      "metadata": {
        "id": "lcqSzYfEqoiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into trn, dev, test\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "dev_and_test = dataset['test'].train_test_split(test_size=0.5)"
      ],
      "metadata": {
        "id": "QbNk-Af1qofU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build final dataset dict\n",
        "dataset = DatasetDict({\n",
        "  'train': dataset['train'],\n",
        "  'dev': dev_and_test['train'],\n",
        "  'test': dev_and_test['test']\n",
        "  })"
      ],
      "metadata": {
        "id": "099v2eAOqxPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# BLEU score increases by 2pts (0.55 to 0.57) when we clean the data of all non-English characters (mainly affects names and places)\n",
        "def clean_data(data):\n",
        "  inputs = data['text_inputs']\n",
        "  inputs = [re.sub(r'[^\\x00-\\x7f]',r'',x) for x in inputs]\n",
        "\n",
        "  labels = data['text_labels']\n",
        "  labels = [re.sub(r'[^\\x00-\\x7f]',r'',x) for x in labels]\n",
        "\n",
        "  return {'text_inputs':inputs,'text_labels':labels}"
      ],
      "metadata": {
        "id": "8P71g6LfqxMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(clean_data,batched=True)"
      ],
      "metadata": {
        "id": "XRJgysy-qxGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in model and relevant hyperparameters"
      ],
      "metadata": {
        "id": "-_ZZw3ZppUxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "UOlgCluepL-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using T5 small model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "\n",
        "# manually add tokens for parentheses (LRB and RRB) to the tokenizer and model\n",
        "special_tokens_dict = {'additional_special_tokens': ['-LRB-', '-RRB-']}\n",
        "tokenizer.add_special_tokens(special_tokens_dict)\n",
        "\n",
        "st_model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)\n",
        "st_model.resize_token_embeddings(len(tokenizer))"
      ],
      "metadata": {
        "id": "oqBTmdZUpL8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing \n",
        "\n",
        "We took guidance from the HF T5 Tutorials - https://huggingface.co/docs/transformers/model_doc/t5"
      ],
      "metadata": {
        "id": "XkwdbS4speIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prefix we chose follows the format and style shown in the original T5 paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "# (see their Appendix D for examples)\n",
        "prefix = \"simplify: \""
      ],
      "metadata": {
        "id": "cVi2XmHWpL5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define max input,target lengths\n",
        "max_input_length = 128\n",
        "max_target_length = 128"
      ],
      "metadata": {
        "id": "ps2WMhCjpL3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_inputs(data):\n",
        "  return tokenizer([prefix + input for input in data['text_inputs']], padding='max_length', max_length=max_input_length, truncation=True, return_tensors='pt')\n",
        "\n",
        "def preprocess_labels(data):\n",
        "  encoding = tokenizer(data['text_labels'], padding='max_length', max_length=max_target_length, truncation=True, return_tensors='pt')\n",
        "  return {'labels': encoding['input_ids']}"
      ],
      "metadata": {
        "id": "ZaWMcX-ypL0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess_inputs,batched=True)\n",
        "dataset = dataset.map(preprocess_labels,batched=True)"
      ],
      "metadata": {
        "id": "C7-enqCQpLyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into trn, dev, and test datasets. Note that we're not using all available data for the sake of time\n",
        "train_dataset = dataset['train'].remove_columns(['text_inputs','text_labels']).shuffle(seed=42).select(range(5000))\n",
        "dev_dataset = dataset['dev'].remove_columns(['text_inputs','text_labels']).shuffle(seed=42).select(range(625))\n",
        "test_dataset = dataset['test'].remove_columns(['text_inputs','text_labels']).shuffle(seed=42).select(range(625))"
      ],
      "metadata": {
        "id": "mm6ygq2NpLvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetune model"
      ],
      "metadata": {
        "id": "Vo1T9scip5jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ],
      "metadata": {
        "id": "jrXClJtppLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "89bd1kCdpLjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import regex as re\n",
        "# converts parentheses tags into parentheses for outputting to website\n",
        "def postprocess_text_for_output(text):\n",
        "  text = re.sub('-LRB-', '(', text)\n",
        "  text = re.sub('-RRB-', ')', text)\n",
        "  return text"
      ],
      "metadata": {
        "id": "DqEpuuD4rTON"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "import textstat\n",
        "\n",
        "# compute rouge, bleu, sari, and readability\n",
        "def my_compute_metrics(eval_pred):\n",
        "  inputs = eval_pred.inputs\n",
        "  labels = eval_pred.label_ids\n",
        "  pred_ids = eval_pred.predictions\n",
        "  if isinstance(pred_ids, tuple):\n",
        "    pred_ids = pred_ids[0]\n",
        "\n",
        "  preds = np.argmax(pred_ids, axis=-1)\n",
        "\n",
        "  # Decode inputs to compute metrics (specifically, SARI requires inputs)\n",
        "  inputs_text = tokenizer.batch_decode(inputs, skip_special_tokens=True)\n",
        "\n",
        "  # Decode predictions to compute metrics\n",
        "  preds_text = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  labels_text = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  preds_text, labels_text = postprocess_text(preds_text, labels_text)\n",
        "  \n",
        "  # init metrics\n",
        "  metric1 = evaluate.load('rouge')\n",
        "  metric2 = evaluate.load('bleu')\n",
        "  metric3 = evaluate.load('sari')\n",
        "\n",
        "  # compute metrics\n",
        "  metrics = metric1.compute(predictions=preds_text, references=labels_text)\n",
        "  metrics['bleu'] = metric2.compute(predictions=preds_text, references=labels_text)['bleu']\n",
        "  metrics['readability'] = sum([textstat.flesch_kincaid_grade(x,) for x in preds_text]) / len(preds_text) #FKGL score\n",
        "  metrics['sari'] = metric3.compute(sources=inputs_text, predictions=preds_text, references=labels_text)['sari']\n",
        "\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "mIJiexzSp-_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 8\n",
        "eval_batch_size = 8"
      ],
      "metadata": {
        "id": "gUd6kC25pLpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='style_transfer_chkpts',\n",
        "    include_inputs_for_metrics=True,   # allows us to compute SARI in compute_metrics fn\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    eval_accumulation_steps=1,\n",
        "    prediction_loss_only=False,\n",
        "    learning_rate=0.0005,\n",
        "    evaluation_strategy='steps',\n",
        "    save_steps=1000,\n",
        "    save_total_limit=3,\n",
        "    remove_unused_columns=True,\n",
        "    run_name='run_final', # Wandb run name\n",
        "    logging_steps=1000, \n",
        "    eval_steps=1000, \n",
        "    logging_first_step=False, \n",
        "    load_best_model_at_end=True, \n",
        "    metric_for_best_model=\"loss\", # loss to eval models\n",
        "    greater_is_better=False \n",
        ")"
      ],
      "metadata": {
        "id": "7cJkZ94IpLml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "  model=st_model,\n",
        "  args=training_args,\n",
        "  train_dataset=train_dataset,\n",
        "  eval_dataset=dev_dataset,\n",
        "  data_collator=data_collator,\n",
        "  compute_metrics=my_compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "h-9mq_7Bp-8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Inqoo27Pp-5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save finetuned model and tokenizer\n",
        "trainer.save_model('best_model_simplify')\n",
        "tokenizer.save_pretrained('best_model_simplify')\n",
        "\n",
        "model = trainer.model"
      ],
      "metadata": {
        "id": "iq2WklanqUoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model"
      ],
      "metadata": {
        "id": "Ab36LG5EqSbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval finetuned model in inference mode\n",
        "test_args = TrainingArguments(\n",
        "  output_dir=\"finetune_simp_eval\",\n",
        "  do_train=False,\n",
        "  do_predict=True,\n",
        "  fp16=True,\n",
        "  per_device_eval_batch_size=8,   \n",
        ")"
      ],
      "metadata": {
        "id": "9ttBkf28pLdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init test trainer\n",
        "test_trainer = Trainer(\n",
        "            model=model, \n",
        "            args=test_args, \n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=my_compute_metrics)"
      ],
      "metadata": {
        "id": "mtrEIQ0dqM9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = test_trainer.predict(test_dataset)\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "mhZQsuQuqNSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}