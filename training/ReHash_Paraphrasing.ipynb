{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKXnCpx7o9Iw"
      },
      "outputs": [],
      "source": [
        "# Got CUDA out of memory errors due to fragmentation, used this line to fix the issue\n",
        "import os\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"max_split_size_mb:512\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read in data\n",
        "\n",
        "Data can be found at: https://quoradata.quora.com/First-Quora-Dataset-Release-Question-Pairs"
      ],
      "metadata": {
        "id": "vaXT4SwJpNbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_json(path_or_buf='quora_duplicates.jsonl.gz', lines=True)"
      ],
      "metadata": {
        "id": "bw5LwxsYpGDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset, DatasetDict\n",
        "data = {'text_inputs':data[0], 'text_labels':data[1]}\n",
        "\n",
        "dataset = Dataset.from_pandas(pd.DataFrame(data=data))"
      ],
      "metadata": {
        "id": "Z3sorI4wpLKv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into train, dev, test\n",
        "dataset = dataset.train_test_split(test_size=0.2)\n",
        "dev_and_test = dataset['test'].train_test_split(test_size=0.5)"
      ],
      "metadata": {
        "id": "8zshMisvpMHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build final dataset dict\n",
        "dataset = DatasetDict({\n",
        "  'train': dataset['train'],\n",
        "  'dev': dev_and_test['train'],\n",
        "  'test': dev_and_test['test']\n",
        "  })"
      ],
      "metadata": {
        "id": "JMvhLTKipMEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filter out samples where input and target are the same\n",
        "dataset = dataset.filter(lambda x: x['text_inputs'] != x['text_labels'])\n",
        "\n",
        "print(dataset)"
      ],
      "metadata": {
        "id": "Wfrl0352pMBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load in model and relevant hyperparameters"
      ],
      "metadata": {
        "id": "-_ZZw3ZppUxZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
      ],
      "metadata": {
        "id": "UOlgCluepL-_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using T5 small model\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\").to(device)"
      ],
      "metadata": {
        "id": "oqBTmdZUpL8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing \n",
        "\n",
        "We took guidance from the HF T5 Tutorials - https://huggingface.co/docs/transformers/model_doc/t5"
      ],
      "metadata": {
        "id": "XkwdbS4speIg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prefix we chose follows the format and style shown in the original T5 paper: https://arxiv.org/pdf/1910.10683.pdf\n",
        "# (see their Appendix D for examples)\n",
        "prefix = \"paraphrase: \""
      ],
      "metadata": {
        "id": "cVi2XmHWpL5y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define max input,target lengths\n",
        "max_input_length = 250\n",
        "max_target_length = 250\n"
      ],
      "metadata": {
        "id": "ps2WMhCjpL3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_inputs(data):\n",
        "  return tokenizer([prefix + input for input in data['text_inputs']], max_length=max_input_length, padding='max_length', truncation=True, return_tensors='pt')\n",
        "\n",
        "def preprocess_labels(data):\n",
        "  encoding = tokenizer(data['text_labels'], padding='max_length', max_length=max_target_length, truncation=True, return_tensors='pt')\n",
        "  return {'labels': encoding['input_ids']}"
      ],
      "metadata": {
        "id": "ZaWMcX-ypL0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.map(preprocess_inputs,batched=True)\n",
        "dataset = dataset.map(preprocess_labels,batched=True)"
      ],
      "metadata": {
        "id": "C7-enqCQpLyJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# split into trn, dev, and test datasets. Note that we're not using all available data for the sake of time\n",
        "train_dataset = dataset['train'].shuffle(seed=42).select(range(5000))\n",
        "dev_dataset = dataset['dev'].shuffle(seed=42).select(range(625))\n",
        "test_dataset = dataset['test'].shuffle(seed=42).select(range(625))"
      ],
      "metadata": {
        "id": "mm6ygq2NpLvX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetune model"
      ],
      "metadata": {
        "id": "Vo1T9scip5jK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer)"
      ],
      "metadata": {
        "id": "jrXClJtppLrN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def postprocess_text(preds, labels):\n",
        "    preds = [pred.strip() for pred in preds]\n",
        "    labels = [[label.strip()] for label in labels]\n",
        "    return preds, labels"
      ],
      "metadata": {
        "id": "89bd1kCdpLjr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import evaluate\n",
        "import numpy as np\n",
        "import textstat\n",
        "\n",
        "# compute rouge, bleu, and readability. Also computes readability of dataset for comparison purposes\n",
        "def my_compute_metrics(eval_pred):\n",
        "  labels = eval_pred.label_ids\n",
        "  pred_ids = eval_pred.predictions\n",
        "  if isinstance(pred_ids, tuple):\n",
        "    pred_ids = pred_ids[0]\n",
        "\n",
        "  preds = np.argmax(pred_ids, axis=-1)\n",
        "\n",
        "  # Decode predictions to compute metrics\n",
        "  preds_text = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
        "\n",
        "  labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
        "  labels_text = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
        "\n",
        "  preds_text, labels_text = postprocess_text(preds_text, labels_text)\n",
        "  \n",
        "  # init metrics\n",
        "  metric1 = evaluate.load('rouge')\n",
        "  metric2 = evaluate.load('bleu')\n",
        "\n",
        "  # compute metrics\n",
        "  metrics = metric1.compute(predictions=preds_text, references=labels_text)\n",
        "  metrics['bleu'] = metric2.compute(predictions=preds_text, references=labels_text)['bleu']\n",
        "  metrics['readability'] = sum([textstat.flesch_kincaid_grade(x) for x in preds_text]) / len(preds_text) #FKGL score\n",
        "  metrics['label_readability'] = sum([textstat.flesch_kincaid_grade(x[0]) for x in labels_text]) / len(labels_text)\n",
        "\n",
        "  return metrics"
      ],
      "metadata": {
        "id": "mIJiexzSp-_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_batch_size = 8\n",
        "eval_batch_size = 8"
      ],
      "metadata": {
        "id": "gUd6kC25pLpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='paraphrase_chkpts',\n",
        "    num_train_epochs=10,\n",
        "    per_device_train_batch_size=train_batch_size,\n",
        "    per_device_eval_batch_size=eval_batch_size,\n",
        "    eval_accumulation_steps=1,\n",
        "    prediction_loss_only=False,\n",
        "    learning_rate=0.001,\n",
        "    evaluation_strategy='steps',\n",
        "    save_steps=1000,\n",
        "    save_total_limit=3,\n",
        "    remove_unused_columns=True,\n",
        "    run_name='run_4_para', # Wandb run name\n",
        "    logging_steps=500, \n",
        "    eval_steps=500, \n",
        "    logging_first_step=False, \n",
        "    load_best_model_at_end=True, \n",
        "    metric_for_best_model=\"bleu\", # loss to eval models\n",
        "    greater_is_better=True \n",
        ")"
      ],
      "metadata": {
        "id": "7cJkZ94IpLml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=dev_dataset,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=my_compute_metrics,\n",
        ")"
      ],
      "metadata": {
        "id": "h-9mq_7Bp-8R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Finetune!\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "Inqoo27Pp-5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save finetuned model and tokenizer\n",
        "trainer.save_model('best_model_paraphrase')\n",
        "tokenizer.save_pretrained('best_model_paraphrase')\n",
        "\n",
        "model = trainer.model"
      ],
      "metadata": {
        "id": "iq2WklanqUoB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate model"
      ],
      "metadata": {
        "id": "Ab36LG5EqSbI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eval finetuned model in inference mode\n",
        "test_args = TrainingArguments(\n",
        "    output_dir=\"finetune_para_eval\",\n",
        "    do_train=False,\n",
        "    do_predict=True,\n",
        "    per_device_eval_batch_size=4,   \n",
        ")"
      ],
      "metadata": {
        "id": "9ttBkf28pLdI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# init test trainer\n",
        "test_trainer = Trainer(\n",
        "            model=model, \n",
        "            args=test_args, \n",
        "            tokenizer=tokenizer,\n",
        "            compute_metrics=my_compute_metrics)"
      ],
      "metadata": {
        "id": "mtrEIQ0dqM9I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_results = test_trainer.predict(test_dataset)\n",
        "print(test_results)"
      ],
      "metadata": {
        "id": "mhZQsuQuqNSo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}